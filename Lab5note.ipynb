{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5 - Voice for our chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyJWT==1.7.1\n",
    "#### the 'ibm_watson' package acts as a wrer. It removes a lot of the hard work, specially for the speech services.\n",
    "#### The 'bs4' (BeautifulSoup4) package enables us to take the output from Watson Assistant and strip away the HTML, to keep only the raw text.\n",
    "#!pip install ibm_watson bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the right modules - For this lab, you'll need to import the following:\n",
    "\n",
    "os - to run commands in the environment via \"os.popen\".\n",
    "\n",
    "glob.glob - to find audio files.\n",
    "\n",
    "bs4 - to extract text from HTML.\n",
    "\n",
    "IPython - to play audio from Watson Text to Speech from within the notebook.\n",
    "\n",
    "ibm_cloud_sdk_core.authenticators.IAMAuthenticator - to help with API Key-based authentication.\n",
    "\n",
    "ibm_watson:\n",
    "\n",
    "    a) SpeechToTextV1 - the Speech to Text service wrer.\n",
    "\n",
    "    b) AssistantV2 - The Assistant service wrer.\n",
    "\n",
    "    c) TextToSpeechV1 - the Text to Speech service wrer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import IPython\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_watson import AssistantV2\n",
    "from ibm_watson import TextToSpeechV1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Speech to Text\n",
    "In order to implement the **Speech to Text** service, you need to first instantiate your service wrer. To do so, create a new instance of 'SpeechToTextV1'. You'll need to pass your API key through the IAMAuthenticator type, as well as the endpoint URL which you can find just under the API Key on the service instance page on IBM Cloud.\n",
    "\n",
    "function called \"recognize_audio()\". This function is simple: it waits for a new audio file to ear in the current working directory (using SPEECH_EXTENSION). Right as it ears, it'll read the file, delete the file from the filesystem, and then pass it to Watson.\n",
    "\n",
    "To parse this JSON, you navigate the hierarchy to get to the transcription that Watson is most confident in. This is how it's done:\n",
    "\n",
    "    \"[\"results\"][0]\" - this will get the first set of results from Watson's response.\n",
    "\n",
    "    \"[\"alternatives\"][0]\" - of all the alternative transcriptions, it'll get the first (most likely) one.\n",
    "\n",
    "    \"[\"transcript\"]\" - of all the data Watson returns, only take the transcript string (\"str\" type in Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_service = SpeechToTextV1(IAMAuthenticator('FJDFwpG1AC42sE-qTnYEvDiUmrrpgHe7y87159y_kNeC'))\n",
    "recognition_service.set_service_url('https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/62d3030a-919d-4c8e-8217-8d1fb9c10b17')\n",
    "SPEECH_EXTENSION = \"*.webm\"\n",
    "SPEECH_AUDIOTYPE = \"audio/webm\"\n",
    "\n",
    "def recognize_audio():\n",
    "    while len(glob(SPEECH_EXTENSION)) == 0:\n",
    "        pass\n",
    "    filename = glob(SPEECH_EXTENSION)[0]\n",
    "    audio_file = open(filename, \"rb\")\n",
    "    os.popen(\"rm \" + filename)\n",
    "    result = recognition_service.recognize(audio=audio_file, content_type=SPEECH_AUDIOTYPE).get_result()\n",
    "    return result[\"results\"][0][\"alternatives\"][0][\"transcript\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversing with Watson Assistant - \n",
    "In order to facilitate the communication with the **Assistant** service, let's define a helper function! This function will take some text from the user, and return Watson's response. Before this function can be defined, we need to instantiate the wrer around the Assistant service itself. In order to do so, create a new instance of \"AssistantV2\". You'll need to provide your **API Key** via an IAMAuthenticator through the \"authenticator\" argument. You'll also need to provide a **version of the AssistantV1 service** - in this case, we're using \"2019-02-28\". You should check the documentation for the current version. You'll also need to define the **Assistant ID of your assistant**. Finally, you'll also need to specify your **endpoint URL** - you can find this on your service instance page right under the API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantV2(version='2019-02-28', authenticator=IAMAuthenticator('Je3R-nT3a2GZyoA06_LLEIMh_O1l_HYVaY29OmvOxNa1'))\n",
    "assistant.set_service_url('https://api.us-south.assistant.watson.cloud.ibm.com/instances/77c27d91-0c1c-4009-9e8d-071e32681d25')\n",
    "ASSISTANT_ID = '2fde6b1d-9c89-45a6-8b7a-a53db38c27f6'\n",
    "session_id = assistant.create_session(assistant_id =ASSISTANT_ID).get_result()[\"session_id\"]\n",
    "\n",
    "def message_assistant(text):\n",
    "    response = assistant.message(assistant_id=ASSISTANT_ID,\n",
    "                                 session_id=session_id,\n",
    "                                 input={'message_type': 'text', 'text': text}).get_result()\n",
    "    return BeautifulSoup(response[\"output\"][\"generic\"][0][\"text\"]).get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we'll go ahead and ask the Assistant to create a new \"session\". \n",
    "With a session, Watson can automatically keep track of the context of a conversation. This means you don't need to handle the context and pass it back and forth with Watson manually. To differentiate between session, you have a session ID, which we store in \"session_id\". You can now define the \"message_assistant\" function. The working of this function is simple:\n",
    "\n",
    "    1 Message the assistant with the user's utterance and the current session ID, and get a JSON response.\n",
    "\n",
    "    2 Return the raw text extracted from the HTML of the first response that Watson returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantV2(version='2019-02-28', authenticator=IAMAuthenticator('Je3R-nT3a2GZyoA06_LLEIMh_O1l_HYVaY29OmvOxNa1'))\n",
    "assistant.set_service_url('https://api.us-south.assistant.watson.cloud.ibm.com/instances/77c27d91-0c1c-4009-9e8d-071e32681d25')\n",
    "ASSISTANT_ID = \"2fde6b1d-9c89-45a6-8b7a-a53db38c27f6\"\n",
    "session_id = assistant.create_session(assistant_id =ASSISTANT_ID).get_result()[\"session_id\"]\n",
    "\n",
    "def message_assistant(text):\n",
    "    response = assistant.message(assistant_id=ASSISTANT_ID,\n",
    "                                 session_id=session_id,\n",
    "                                 input={'message_type': 'text', 'text': text}).get_result()\n",
    "    return BeautifulSoup(response[\"output\"][\"generic\"][0][\"text\"]).get_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hearing Watson's response - \n",
    "To enable a truly end-to-end intuitive and interactive experience, let's use **Text to Speech** to synthesize audio and have Watson speak! Start by initializing the \"TextToSpeechV1\" wrer. Pass it your API Key through an IAMAuthenticator, and your API endpoint, which you can find right under the API Key in your service dashboard on IBM Cloud. Then, define a new function called \"speak_text\". This is what it'll do:\n",
    "\n",
    "    1 Open a new file \"temp.wav\".\n",
    "\n",
    "    2 Take the text that Watson needs to speak and pass it to the \"synthesisservice.synthesize()\" function. Tell it we're passing a WAV file, and tell it we want the \"en-US_AllisonV3Voice\" voice. You can see more voices [here](https://cloud.ibm.com/apidocs/text-to-speech?cm_mmc=Email_Newsletter--DeveloperEd%2BTech--WWWW--SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-CB0106EN-SkillsNetwork-20719128&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ).\n",
    "\n",
    "    3 Write Watson's response to the \"temp.wav\" file.\n",
    "\n",
    "    4 Play the \"temp.wav\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_service = TextToSpeechV1(IAMAuthenticator('9ikPoYJv4fNVPM9Wu-8JCLM0AryW_ANGt97YxZZCnfRs'))\n",
    "synthesis_service.set_service_url('https://api.us-south.text-to-speech.watson.cloud.ibm.com/instances/6e6b30fe-6a58-4216-bdfe-2a286a39eba9')\n",
    "\n",
    "def speak_text(text):\n",
    "    with open('temp.wav', 'wb') as audio_file:\n",
    "        response = synthesis_service.synthesize(text, accept='audio/wav', voice=\"en-US_AllisonV3Voice\").get_result()\n",
    "        audio_file.write(response.content)\n",
    "    return IPython.display.Audio(\"temp.wav\", autoplay=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the pieces together - \n",
    "Because of the way these functions work, putting them together is as easy as chaining them together! By calling \"recognize_audio()\", you're waiting for the user to provide some input. Then, that is passed to the \"message_assistant()\" function. The output of that function is passed to \"speak_text\", which provides output to the user! \n",
    "\n",
    "### To interact with the chatbot in this lab, simply run this cell for every utterance. To be specific:\n",
    "\n",
    "    1 Run the following cell.\n",
    "\n",
    "    2 Record audio.\n",
    "\n",
    "    3 Wait until you hear Watson's response\n",
    "\n",
    "    4 Until you're done, repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_text(message_assistant(recognize_audio()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all! Now, by running this cell every time you wish to speak to Watson, you'll be able to interact in a natural, vocal manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
